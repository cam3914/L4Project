{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [20:45:43] Enabling RDKit 2019.09.1 jupyter extensions\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import rdkit.Chem.MolStandardize\n",
    "import rdkit.Chem.MolStandardize.rdMolStandardize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyteomics import mgf\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import *\n",
    "import operator\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## Check GPU available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_data = fetch_data(\"unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find most common large peak\n",
    "# peaks = count_peaks(unique_data, 0.9)\n",
    "# max_peak = max(peaks.items(), key=operator.itemgetter(1))[0]\n",
    "max_peak = 121\n",
    "\n",
    "##### predict only bin 121\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i, mol in unique_data.iterrows():\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        sys.stdout.write(\"Binning: %d   \\r\" % (i) )\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    x_data.append(mol['fingerprint'])\n",
    "    y_data.append(mol['normed_binned'][max_peak])\n",
    "    \n",
    "    \n",
    "split = int(0.8 * len(x_data))\n",
    "    \n",
    "x_train = np.array(x_data[:split])\n",
    "y_train = np.array(y_data[:split])\n",
    "\n",
    "x_test = np.array(x_data[split:])\n",
    "y_test = np.array(y_data[split:])\n",
    "\n",
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First working MLP on dense data\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(units=1024, activation='sigmoid', input_shape=(2048,)))\n",
    "# model.add(Dense(units=64, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "# sgd = optimizers.SGD(lr=0.01, nesterov=True);\n",
    "# model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "\n",
    "\n",
    "### First working MLP on sparse data\n",
    "\n",
    "# Lrelu = keras.layers.LeakyReLU(alpha=0.3)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(units=2048, activation=Lrelu, input_shape=(2048,)))\n",
    "# model.add(Dense(units=128, activation=Lrelu))\n",
    "# model.add(Dense(units=1, activation=Lrelu))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae', 'mape', 'cosine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=1024, activation='relu', input_shape=(2048,)))\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='sgd', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=32, verbose=1)\n",
    "labels = model.metrics_names\n",
    "\n",
    "for i in range(len(results)):\n",
    "    print(labels[i], results[i])\n",
    "    \n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(np.array(x_test))\n",
    "print(np.all(preds == 0.0))\n",
    "for i, y in enumerate(preds):\n",
    "    print(y, \" => \", y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_data = []\n",
    "y2_data = []\n",
    "\n",
    "for i, mol in unique_data.iterrows():\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        sys.stdout.write(\"Collecting: %d   \\r\" % (i) )\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    x2_data.append(mol['fingerprint'])\n",
    "    y2_data.append(mol['normed_binned'][50:100])\n",
    "    \n",
    "    \n",
    "split = int(0.8 * len(x2_data))\n",
    "    \n",
    "x2_train = np.array(x2_data[:split])\n",
    "y2_train = np.array(y2_data[:split])\n",
    "\n",
    "x2_test = np.array(x2_data[split:])\n",
    "y2_test = np.array(y2_data[split:])\n",
    "\n",
    "len(x2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = Sequential()\n",
    "# model2.add(Dense(units=2048, activation='relu', input_shape=(2048,)))\n",
    "# #model2.add(Dropout(0.1))\n",
    "# #model2.add(Dense(units=2000, activation='relu'))\n",
    "# #model2.add(Dropout(0.1))\n",
    "# #model2.add(Dense(units=2000, activation='relu'))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Dense(units=2000, activation='relu'))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Dense(units=2000, activation='relu'))\n",
    "# model2.add(Dropout(0.1))\n",
    "# model2.add(Dense(units=50, activation='relu'))\n",
    "    \n",
    "adam = keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "params_array = [\n",
    "    #{'optimizer': 'sgd', 'loss': 'mean_absolute_percentage_error'},\n",
    "    {'optimizer': adam, 'loss': 'mean_absolute_percentage_error'},\n",
    "    #{'optimizer': 'rmsprop', 'loss': 'mean_absolute_percentage_error'},\n",
    "    #{'optimizer': 'adamax', 'loss': 'mean_absolute_percentage_error'},\n",
    "]\n",
    "\n",
    "models = []\n",
    "histories = []\n",
    "\n",
    "for i, params in enumerate(params_array):\n",
    "    print(params['optimizer'], params['loss'])\n",
    "    \n",
    "    models.append(Sequential())\n",
    "    models[i].add(Dense(units=2048, activation='relu', input_shape=(2048,)))\n",
    "    #models2.add(Dropout(0.1))\n",
    "    #models2.add(Dense(units=2000, activation='relu'))\n",
    "    #models2.add(Dropout(0.1))\n",
    "    #models2.add(Dense(units=2000, activation='relu'))\n",
    "    models[i].add(Dropout(0.1))\n",
    "    models[i].add(Dense(units=2000, activation='relu'))\n",
    "    models[i].add(Dropout(0.1))\n",
    "    models[i].add(Dense(units=2000, activation='relu'))\n",
    "    models[i].add(Dropout(0.1))\n",
    "    models[i].add(Dense(units=50, activation='relu'))\n",
    "    \n",
    "    models[i].compile(loss='mean_absolute_percentage_error', optimizer=adam)\n",
    "    histories.append(models[i].fit(x2_train, y2_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = model2.evaluate(x2_test, y2_test, batch_size=32, verbose=1)\n",
    "labels2 = model2.metrics_names\n",
    "\n",
    "for i in range(len(results2)):\n",
    "    print(labels2[i], results2[i])\n",
    "    \n",
    "# Plot training & validation loss values\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model loss (MSE)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 3\n",
    "show_spectrum(unique_data.iloc[base])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(\"duplicates.txt\", \"r\")\n",
    "# f1 = file.readlines()\n",
    "# len(f1)\n",
    "\n",
    "# distinct = []\n",
    "# unique_pairs = []\n",
    "# for string in f1:\n",
    "#     matched = string.split(\", \")\n",
    "#     x = int(matched[0])\n",
    "#     y = int(matched[1])\n",
    "    \n",
    "#     pair = (x, y)\n",
    "#     reverse = (y, x)\n",
    "    \n",
    "#     if x not in distinct:\n",
    "#         distinct.append(x)\n",
    "        \n",
    "#     if pair not in unique_pairs and reverse not in unique_pairs:\n",
    "#         unique_pairs.append(pair)\n",
    "\n",
    "        \n",
    "# print(len(distinct))\n",
    "# print(len(unique_pairs))\n",
    "\n",
    "## combinations w/out repitition =   n! / r!(n-r)! == 33936441\n",
    "## 2326 / 33936441 * 100 = 0.006853989%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_spectrum(unique_data.iloc[0], unique_data.iloc[775])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions50 = models[0].predict(x2_test)\n",
    "\n",
    "for i in range(len(y2_test)):\n",
    "    if not np.all(predictions50[i] == 0):        \n",
    "        compare_bins(y2_test[i], predictions50[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "xn_data = []\n",
    "yn_data = []\n",
    "\n",
    "for i, mol in unique_data.iterrows():\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        sys.stdout.write(\"Collecting: %d   \\r\" % (i) )\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    xn_data.append(mol['fingerprint'])\n",
    "    yn_data.append(mol['normed_binned'][50:100])\n",
    "    \n",
    "    \n",
    "split = int(0.8 * len(xn_data))\n",
    "    \n",
    "xn_train = np.array(xn_data[:split])\n",
    "yn_train = np.array(yn_data[:split])\n",
    "\n",
    "xn_test = np.array(xn_data[split:])\n",
    "yn_test = np.array(yn_data[split:])\n",
    "\n",
    "print(len(x2_data))\n",
    "\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(units=2048, activation='relu', input_shape=(2048,)))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(units=2000, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(units=2000, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(units=2000, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(units=2000, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(units=2000, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(units=2000, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(units=50, activation='relu'))\n",
    "\n",
    "model2.compile(loss='mean_absolute_percentage_error', optimizer=adam)\n",
    "history2 = model2.fit(x2_train, y2_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_counts = np.zeros(3000)\n",
    "\n",
    "for i, mol in unique_data.iterrows():\n",
    "    for j, val in enumerate(mol['normed_binned']):\n",
    "        if val > 0.01:\n",
    "            bin_counts[j] = bin_counts[j] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(bin_counts[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "nonzero = False\n",
    "while not nonzero:\n",
    "    if bin_counts[i] > 0:\n",
    "        print(bin_counts[i])\n",
    "        nonzero = True\n",
    "    i += 1\n",
    "    \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dissertation)",
   "language": "python",
   "name": "dissertation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
